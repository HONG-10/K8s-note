# Kubernetes 설치

> kubeadm을 활용하여 설치

* OS : CentOS
* Container Engine : docker-ce 

## 1. Kubernetes 설치를 위한 OS 환경 구성

### 1.1 OS 제약사항(보안 설정) 해제 후 설치

```sh
############################# selinux
$ setenforce 0
$ sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/selinux/config
$ sestatus
    > ...
    > Current mode:                   permissive # 확인
    > Mode from config file:          permissive # 확인

############################# firewall
$ systemctl stop firewalld.service && systemctl disable firewalld.service
$ systemctl list-unit-files | grep firewall
    > firewalld.service   disabled
```

### 1.2 swap 설정

> swap 주석 처리 근거

* 가상 메모리: disk ro file 공간 -> idle process가 들어온 상태
* 물리적 메모리 부족 시, active process에 할당
* But, K8S => swap diable (pod는 절대 swap에 가지 않는다)
* Why? container = process 이기 때문

```sh
$ swapoff -a
# /etc/fstab --> swap 영역 주석 처리
$ sed '//dev/mapper/centos_k8s--master-swap swap/ s/^/#/' /etc/fstab
```

### 1.3 ip forwording

> 커널 파라미터 구성 요구

```sh
$ echo '1' > /proc/sys/net/ipv4/ip_forward
$ cat /proc/sys/net/ipv4/ip_forward
    > 1     # 1 확인
```

### 1.4 iptables 방화벽

> bridge network를 통해 들어온 트래픽의 정확한 전달 ==netfilter

```sh
$ modprobe br_netfilter
$ lsmod | grep br_netfilter
    > br_netfilter           22256  0
    > bridge                151336  1 br_netfilter
$ vi /etc/modules-load.d/k8s.conf
$ cat /etc/modules-load.d/k8s.conf
    > br_netfilter
$ vi /etc/sysctl.d/k8s.conf
    > net.bridge.bridge-nf-call-ip6tables = 1
    > net.bridge.bridge-nf-call-iptables = 1
$ sysctl --system
    > * Applying /usr/lib/sysctl.d/00-system.conf ...
    > net.bridge.bridge-nf-call-ip6tables = 0
    > net.bridge.bridge-nf-call-iptables = 0
    > net.bridge.bridge-nf-call-arptables = 0
    > * Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
    > kernel.yama.ptrace_scope = 0
    > * Applying /usr/lib/sysctl.d/50-default.conf ...
    > kernel.sysrq = 16
    > kernel.core_uses_pid = 1
    > net.ipv4.conf.default.rp_filter = 1
    > net.ipv4.conf.all.rp_filter = 1
    > net.ipv4.conf.default.accept_source_route = 0
    > net.ipv4.conf.all.accept_source_route = 0
    > net.ipv4.conf.default.promote_secondaries = 1
    > net.ipv4.conf.all.promote_secondaries = 1
    > fs.protected_hardlinks = 1
    > fs.protected_symlinks = 1
    > * Applying /usr/lib/sysctl.d/60-libvirtd.conf ...
    > fs.aio-max-nr = 1048576
    > * Applying /etc/sysctl.d/99-sysctl.conf ...   # 확인
    > * Applying /etc/sysctl.d/k8s.conf ...         # 확인
    > net.bridge.bridge-nf-call-ip6tables = 1       # 확인
    > net.bridge.bridge-nf-call-iptables = 1        # 확인
    > * Applying /etc/sysctl.conf ...               # 확인
```

## 2. Kubernetes Container Engine 설치
### 2.1 Docker Engine설치
* docker-ce (Community Edition) **
* docekr-ee (Enterprise Edition)

```sh
# 전용 yum repository 구성
$ yum -y install yum-utils device-mapper-persistent-data lvm2
$ yum-config-manager -add repo https://download.docker.com/linux/centos/docker-ce.repo
$ cd /etc/yum.repos.d/
$ yum list | grep docker-ce
$ yum list docker-ce --showduplicates
    > ...
    > docker-ce.x86_64           3:20.10.17-3.el7                   docker-ce-stable

$ yum -y install docker-ce
$ mkdir /etc/docker
$ vi /etc/docker/daemon.json
```

```json
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "log-driver": "json-file",
    "log-opts": {
        "max-size": "100m"
    },
    "storage-driver": "overlay2",
    "storage-opts": ["overlay2.override_kernel_check=true"]
}
```
### 2.2 Docker Service 등록
```sh
$ mkdir -p /etc/systemd/system/docker.service.d
$ systemctl daemon-reload
$ systemctl enable docker --now
$ systemctl start docker
$ systemctl status docker
$ docker version
$ docker info

# docker 테스트
$ docker run -itd -p 8000:80 --name=webserver nginx:1.21-alpine 
$ docker ps
$ curl localhost:8000
$ docker stop webserver
$ docker rm webserver
$ docker rmi nginx:1.21-alpine

$ yum -y update
```

## 3. Kubernetes 설치
### 3.1 Repository 생성
```sh
$ yum -y update
    > [failure: ~~ 일 때] 
$ sed 's/gpgcheck=1/gpgcheck=0/g' /etc/yum.repos.d/kubernetes.repo
$ sed 's/repo_gpgcheck=1/repo_gpgcheck=0/g' /etc/yum.repos.d/kubernetes.repo
$ yum -y update

$ yum repolist
    > ...
    > kubernetes                             Kubernetes                                     832
$ yum list | grep kubernetes
$ yum list kubeadm --showduplicates | grep 1.23
$ yum -y install kubeadm-1.23.5 kubelet-1.23.5 kubectl-1.23.5 --disableexcludes=kubernetes
$ yum list installed | grep kubernetes
$ poweroff
```

### 3.2 Kubernetes Tools 생성

## 4. Master Node to Worker Node 복제

### 4.1 VB 복제 작업
### 4.2 ip 변경 리눅스 제어판 8 100 -> 101, 102, 103
### 4.3 hostname 변경 & DNS 설정
```sh
# Master, Worker 모든 Node에서 실행 
HOST_NAME='k8s-node1'
$ hostnamectl set-hostname "${HOST_NAME}"
$ tee /etc/hosts << EOF
192.168.56.100  k8s-master
192.168.56.101  k8s-node1
192.168.56.102  k8s-node2
192.168.56.103  k8s-node3
EOF

# known_hosts 생성
$ ssh k8s-node1     # 각 node 돌아가면서 모두 수행
$ cat ~/.ssh/known_hosts
```

## 5. Bootstraping | kubeamd init
```sh
$ kubeadm init --pod-network-cidr=10.96.0.0/12 --apiserver-advertise-address=192.168.56.100
###############################################
################ 아래 내용 기록 ################
############################################### 
> I0816 17:14:06.640064    9964 version.go:255] remote version is much newer: v1.24.3; falling back to: stable-1.23
> [init] Using Kubernetes version: v1.23.9
> [preflight] Running pre-flight checks
> [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
> [preflight] Pulling images required for setting up a Kubernetes cluster
> [preflight] This might take a minute or two, depending on the speed of your internet connection
> [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
> [certs] Using certificateDir folder "/etc/kubernetes/pki"
> [certs] Generating "ca" certificate and key
> [certs] Generating "apiserver" certificate and key
> [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.56.100]
> [certs] Generating "apiserver-kubelet-client" certificate and key
> [certs] Generating "front-proxy-ca" certificate and key
> [certs] Generating "front-proxy-client" certificate and key
> [certs] Generating "etcd/ca" certificate and key
> [certs] Generating "etcd/server" certificate and key
> [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.56.100 127.0.0.1 ::1]
> [certs] Generating "etcd/peer" certificate and key
> [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.56.100 127.0.0.1 ::1]
> [certs] Generating "etcd/healthcheck-client" certificate and key
> [certs] Generating "apiserver-etcd-client" certificate and key
> [certs] Generating "sa" key and public key
> [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
> [kubeconfig] Writing "admin.conf" kubeconfig file
> [kubeconfig] Writing "kubelet.conf" kubeconfig file
> [kubeconfig] Writing "controller-manager.conf" kubeconfig file
> [kubeconfig] Writing "scheduler.conf" kubeconfig file
> [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
> [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
> [kubelet-start] Starting the kubelet
> [control-plane] Using manifest folder "/etc/kubernetes/manifests"
> [control-plane] Creating static Pod manifest for "kube-apiserver"
> [control-plane] Creating static Pod manifest for "kube-controller-manager"
> [control-plane] Creating static Pod manifest for "kube-scheduler"
> [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
> [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". Thi                     s can take up to 4m0s
> [apiclient] All control plane components are healthy after 21.538767 seconds
> [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
> [kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
> NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduat                     es to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
> [upload-certs] Skipping phase. Please see --upload-certs
> [mark-control-plane] Marking the node k8s-master as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node                     -role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
> [mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
> [bootstrap-token] Using token: y32zrh.15c4rw3nufkpj6aj
> [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
> [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
> [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate cred                     entials
> [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
> [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
> [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
> [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
> [addons] Applied essential addon: CoreDNS
> [addons] Applied essential addon: kube-proxy
>
> Your Kubernetes control-plane has initialized successfully!
> 
> To start using your cluster, you need to run the following as a regular user:
>
    > mkdir -p $HOME/.kube                                          ########################################
    > sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config      ############### 필수 작업 ###############
    > sudo chown $(id -u):$(id -g) $HOME/.kube/config               ########################################
>
> Alternatively, if you are the root user, you can run:
>
    > export KUBECONFIG=/etc/kubernetes/admin.conf                  ############### 필수 작업 ###############
>
> You should now deploy a pod network to the cluster.
> Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
>   https://kubernetes.io/docs/concepts/cluster-administration/addons/
> 
> Then you can join any number of worker nodes by running the following on each as root:
>
> kubeadm join 192.168.56.100:6443 --token y32zrh.15c4rw3nufkpj6aj \
>        --discovery-token-ca-cert-hash sha256:40fff0226e4b0a3a6c381847902a1b5e90da436d140c40899c033719bf05dbe4     ############### Join Key ###############

# Initial Setting (필수 작업)
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
$ export KUBECONFIG=/etc/kubernetes/admin.conf
$ . ~/.bash_profile

# kubectl bash completion 등록
$ yum -y install bash-completion
$ source <(kubectl completion bash)
$ echo "source <(kubectl completion bash)" >> ~/.bashrc
```

## 6. Node Join | kubeadm join
```sh
# 모든 Node에서 수행
$ kubeadm join 192.168.56.100:6443 --token y32zrh.15c4rw3nufkpj6aj \
        --discovery-token-ca-cert-hash sha256:40fff0226e4b0a3a6c381847902a1b5e90da436d140c40899c033719bf05dbe4

$ kubectl get nodes                     # Join 확인
$ kubectl get pod --all-namespaces      # 필수 Object 확인
```