# Cluster 구성

> CentOS 환경에서 docker-ce를 활용하여 설치

## 1. Master Node to Worker Node 복제
```sh
$ vi /etc/yum.repos.d/kubernetes.repo
```

```conf
gpgcheck=1 -> 0
repo_gpgcheck=1 -> 0
```
```sh
$ yum -y update
> # == fail 
> # failure: repodata/repomd.xml from kubernetes: [Errno 256] No more mirrors to try.
> # https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/repomd.xml: [Errno -1] repomd.xml signature could not be verified for kubernetes
$ yum repolist
> ...
> kubernetes                             Kubernetes                                     832
$ yum list | grep kubernetes
$ yum list kubeadm --showduplicates | grep 1.23 # --showduplicates 중복 배제
$ yum -y install kubeadm-1.23.5 kubelet-1.23.5 kubectl-1.23.5 --disableexcludes=kubernetes
$ yum list installed | grep kubernetes
$ poweroff
```

### 1.1 VB 복제 작업
### 1.2 ip 변경 리눅스 제어판 8 100 -> 101, 102, 103
### 1.3 hostname 변경 & DNS 설정
```sh
$ hostnamectl set-hostname 'k8s-node1'
$ vi /etc/hosts
```
```conf
...
192.168.56.100  k8s-master
192.168.56.101  k8s-node1
192.168.56.102  k8s-node2
192.168.56.103  k8s-node3
```
> known_hosts 생성
```sh
$ ssh k8s-node1
exit
$ ssh k8s-node2
exit
$ cat .ssh/known_hosts
```

## 2. Bootstraping | kubeamd init
```sh
$ kubeadm init --pod-network-cidr=10.96.0.0/12 --apiserver-advertise-address=192.168.56.100

> I0816 17:14:06.640064    9964 version.go:255] remote version is much newer: v1.24.3; falling back to: stable-1.23
> [init] Using Kubernetes version: v1.23.9
> [preflight] Running pre-flight checks
> [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
> [preflight] Pulling images required for setting up a Kubernetes cluster
> [preflight] This might take a minute or two, depending on the speed of your internet connection
> [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
> [certs] Using certificateDir folder "/etc/kubernetes/pki"
> [certs] Generating "ca" certificate and key
> [certs] Generating "apiserver" certificate and key
> [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.56.100]
> [certs] Generating "apiserver-kubelet-client" certificate and key
> [certs] Generating "front-proxy-ca" certificate and key
> [certs] Generating "front-proxy-client" certificate and key
> [certs] Generating "etcd/ca" certificate and key
> [certs] Generating "etcd/server" certificate and key
> [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.56.100 127.0.0.1 ::1]
> [certs] Generating "etcd/peer" certificate and key
> [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.56.100 127.0.0.1 ::1]
> [certs] Generating "etcd/healthcheck-client" certificate and key
> [certs] Generating "apiserver-etcd-client" certificate and key
> [certs] Generating "sa" key and public key
> [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
> [kubeconfig] Writing "admin.conf" kubeconfig file
> [kubeconfig] Writing "kubelet.conf" kubeconfig file
> [kubeconfig] Writing "controller-manager.conf" kubeconfig file
> [kubeconfig] Writing "scheduler.conf" kubeconfig file
> [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
> [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
> [kubelet-start] Starting the kubelet
> [control-plane] Using manifest folder "/etc/kubernetes/manifests"
> [control-plane] Creating static Pod manifest for "kube-apiserver"
> [control-plane] Creating static Pod manifest for "kube-controller-manager"
> [control-plane] Creating static Pod manifest for "kube-scheduler"
> [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
> [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". Thi                     s can take up to 4m0s
> [apiclient] All control plane components are healthy after 21.538767 seconds
> [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
> [kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
> NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduat                     es to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
> [upload-certs] Skipping phase. Please see --upload-certs
> [mark-control-plane] Marking the node k8s-master as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node                     -role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
> [mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
> [bootstrap-token] Using token: y32zrh.15c4rw3nufkpj6aj
> [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
> [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
> [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate cred                     entials
> [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
> [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
> [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
> [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
> [addons] Applied essential addon: CoreDNS
> [addons] Applied essential addon: kube-proxy
>
> Your Kubernetes control-plane has initialized successfully!
> 
> To start using your cluster, you need to run the following as a regular user:
>
> mkdir -p $HOME/.kube                                          ########################################
> sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config      ############### 필수 작업 ###############
> sudo chown $(id -u):$(id -g) $HOME/.kube/config               ########################################
>
> Alternatively, if you are the root user, you can run:
>
> export KUBECONFIG=/etc/kubernetes/admin.conf                  ############### 필수 작업 ###############
>
> You should now deploy a pod network to the cluster.
> Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
>   https://kubernetes.io/docs/concepts/cluster-administration/addons/
> 
> Then you can join any number of worker nodes by running the following on each as root:
>
> kubeadm join 192.168.56.100:6443 --token y32zrh.15c4rw3nufkpj6aj \                                                 ############### Join Key ###############    
>        --discovery-token-ca-cert-hash sha256:40fff0226e4b0a3a6c381847902a1b5e90da436d140c40899c033719bf05dbe4
```

> Initial Setting
```sh
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
$ export KUBECONFIG=/etc/kubernetes/admin.conf
```

## 3. Node Join
```sh
$ kubeadm join 192.168.56.100:6443 --token y32zrh.15c4rw3nufkpj6aj \
        --discovery-token-ca-cert-hash sha256:40fff0226e4b0a3a6c381847902a1b5e90da436d140c40899c033719bf05dbe4
```
* kube-proxy = docker-proxy (proxy: 전달)
* docker는 local용
* docker -p(publish : 게시) 8001:80
* NAT = 호스트의 IP가 container의 IP로 변경
* NAPT = 사용자가 입력한 포트 8001은 80으로 전달 시킨다 포트포워딩과 비슷
* 이는 k8s는 각 노드의 kube-proxy가 수행!
* 이를 remote, node 간 수행

## 4. Networt Plugin (Calico)
* Coredns Pending(보류) 해지
* k8s 지원 네트워크 플러그인 약 10개 
* https://kubernetes.io/docs/concepts/cluster-administration/addons/
* ip forwording 3 Layer

* network plugin download & apply
```sh
$ curl -O https://docs.projectcalico.org/manifests/calico.yaml
$ kubectl apply -f calico.yaml
# $ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```
